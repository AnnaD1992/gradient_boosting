{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-12T10:30:40.714819Z","iopub.execute_input":"2022-05-12T10:30:40.715499Z","iopub.status.idle":"2022-05-12T10:30:41.668336Z","shell.execute_reply.started":"2022-05-12T10:30:40.715408Z","shell.execute_reply":"2022-05-12T10:30:41.667302Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data= pd.read_csv('../input/seattleWeather_1948-2017.csv')\ndata=data.drop([\"PRCP\"],axis=1)\n\ndata.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-05-12T10:32:46.482373Z","iopub.execute_input":"2022-05-12T10:32:46.482778Z","iopub.status.idle":"2022-05-12T10:32:46.523790Z","shell.execute_reply.started":"2022-05-12T10:32:46.482717Z","shell.execute_reply":"2022-05-12T10:32:46.522836Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"pd.options.display.float_format = '{:.2f}%'.format ## Set a percentage view as a default for Float\ndef missing_values(n):\n    df=pd.DataFrame()\n    df[\"missing, %\"]=data.isnull().sum()*100/len(data.isnull())\n    df[\"missing, num\"]=data.isnull().sum()\n    return df.sort_values(by=\"missing, %\", ascending=False)\nmissing_values(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:32:48.825792Z","iopub.execute_input":"2022-05-12T10:32:48.826387Z","iopub.status.idle":"2022-05-12T10:32:48.853406Z","shell.execute_reply.started":"2022-05-12T10:32:48.826318Z","shell.execute_reply":"2022-05-12T10:32:48.852309Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"data.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:41.886995Z","iopub.execute_input":"2022-05-12T10:30:41.887354Z","iopub.status.idle":"2022-05-12T10:30:41.900676Z","shell.execute_reply.started":"2022-05-12T10:30:41.887287Z","shell.execute_reply":"2022-05-12T10:30:41.899835Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:41.902999Z","iopub.execute_input":"2022-05-12T10:30:41.903409Z","iopub.status.idle":"2022-05-12T10:30:41.914414Z","shell.execute_reply.started":"2022-05-12T10:30:41.903368Z","shell.execute_reply":"2022-05-12T10:30:41.913508Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"missing_values(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:41.918027Z","iopub.execute_input":"2022-05-12T10:30:41.918668Z","iopub.status.idle":"2022-05-12T10:30:41.940110Z","shell.execute_reply.started":"2022-05-12T10:30:41.918449Z","shell.execute_reply":"2022-05-12T10:30:41.939207Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Dealing with missing values for the machine learning : https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e","metadata":{}},{"cell_type":"code","source":" ","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:41.941390Z","iopub.execute_input":"2022-05-12T10:30:41.941896Z","iopub.status.idle":"2022-05-12T10:30:41.945605Z","shell.execute_reply.started":"2022-05-12T10:30:41.941841Z","shell.execute_reply":"2022-05-12T10:30:41.944753Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"x=data.iloc[:,1:-1].values\nx","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:32:54.065603Z","iopub.execute_input":"2022-05-12T10:32:54.065952Z","iopub.status.idle":"2022-05-12T10:32:54.072978Z","shell.execute_reply.started":"2022-05-12T10:32:54.065901Z","shell.execute_reply":"2022-05-12T10:32:54.072322Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"y=data.iloc[:,-1].values\ny","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:32:56.633793Z","iopub.execute_input":"2022-05-12T10:32:56.634330Z","iopub.status.idle":"2022-05-12T10:32:56.640293Z","shell.execute_reply.started":"2022-05-12T10:32:56.634272Z","shell.execute_reply":"2022-05-12T10:32:56.639661Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Encoding of the Rain prediction column","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny=le.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:32:59.937669Z","iopub.execute_input":"2022-05-12T10:32:59.938297Z","iopub.status.idle":"2022-05-12T10:32:59.947478Z","shell.execute_reply.started":"2022-05-12T10:32:59.938244Z","shell.execute_reply":"2022-05-12T10:32:59.946435Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:42.588516Z","iopub.execute_input":"2022-05-12T10:30:42.588887Z","iopub.status.idle":"2022-05-12T10:30:42.593284Z","shell.execute_reply.started":"2022-05-12T10:30:42.588720Z","shell.execute_reply":"2022-05-12T10:30:42.592715Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Split the dataset into Training set and Test Set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:33:03.929850Z","iopub.execute_input":"2022-05-12T10:33:03.930195Z","iopub.status.idle":"2022-05-12T10:33:03.939606Z","shell.execute_reply.started":"2022-05-12T10:33:03.930138Z","shell.execute_reply":"2022-05-12T10:33:03.938306Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:33:05.489522Z","iopub.execute_input":"2022-05-12T10:33:05.490172Z","iopub.status.idle":"2022-05-12T10:33:05.496155Z","shell.execute_reply.started":"2022-05-12T10:33:05.490113Z","shell.execute_reply":"2022-05-12T10:33:05.494785Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:33:08.017392Z","iopub.execute_input":"2022-05-12T10:33:08.017945Z","iopub.status.idle":"2022-05-12T10:33:08.023650Z","shell.execute_reply.started":"2022-05-12T10:33:08.017890Z","shell.execute_reply":"2022-05-12T10:33:08.022636Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train[:]=sc.fit_transform(x_train[:])\nx_test[:] = sc.transform(x_test[:])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:42.672947Z","iopub.execute_input":"2022-05-12T10:30:42.673317Z","iopub.status.idle":"2022-05-12T10:30:42.688106Z","shell.execute_reply.started":"2022-05-12T10:30:42.673279Z","shell.execute_reply":"2022-05-12T10:30:42.687174Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:42.689269Z","iopub.execute_input":"2022-05-12T10:30:42.689676Z","iopub.status.idle":"2022-05-12T10:30:42.696584Z","shell.execute_reply.started":"2022-05-12T10:30:42.689621Z","shell.execute_reply":"2022-05-12T10:30:42.695866Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"x_test","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:42.697884Z","iopub.execute_input":"2022-05-12T10:30:42.698140Z","iopub.status.idle":"2022-05-12T10:30:42.710530Z","shell.execute_reply.started":"2022-05-12T10:30:42.698092Z","shell.execute_reply":"2022-05-12T10:30:42.709518Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The main difference between Regression and Classification algorithms that Regression algorithms are used to predict the continuous values such as price, salary, age, etc. and Classification algorithms are used to predict/Classify the discrete values such as Male or Female, True or False, Spam or Not Spam, etc. (https://www.javatpoint.com/regression-vs-classification-in-machine-learning#:~:text=The%20main%20difference%20between%20Regression,Spam%20or%20Not%20Spam%2C%20etc.) \n","metadata":{}},{"cell_type":"markdown","source":"# GradientBoostingClassifier ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:54:46.120814Z","iopub.execute_input":"2022-04-29T09:54:46.121231Z","iopub.status.idle":"2022-04-29T09:54:46.125861Z","shell.execute_reply.started":"2022-04-29T09:54:46.121165Z","shell.execute_reply":"2022-04-29T09:54:46.124551Z"}}},{"cell_type":"markdown","source":"Gradient Boosting ensemble is an ensemble created from decision trees added sequentially to the model. Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n\nThere are three types of enhancements to basic gradient boosting that can improve performance:\n\n- Tree Constraints: such as the depth of the trees and the number of trees used in the ensemble.\n- Weighted Updates: such as a learning rate used to limit how much each tree contributes to the ensemble.\n- Random sampling: such as fitting trees on random subsets of features and samples\nhttps://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/","metadata":{}},{"cell_type":"markdown","source":"## Gradient Boosting for Classification\n","metadata":{}},{"cell_type":"code","source":"# evaluate gradient boosting algorithm for classification\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\n#define the model\nmodel = GradientBoostingClassifier()\n# define the evaluation method\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n\n\nn_scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n#the mean and standard deviation of the accuracy of the model across all repeats and folds.","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:42.711587Z","iopub.execute_input":"2022-05-12T10:30:42.711953Z","iopub.status.idle":"2022-05-12T10:30:53.046480Z","shell.execute_reply.started":"2022-05-12T10:30:42.711912Z","shell.execute_reply":"2022-05-12T10:30:53.045587Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"n_scores = cross_val_score(model, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n#The mean absolute error (MAE) larger negative MAE are better\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:30:53.047878Z","iopub.execute_input":"2022-05-12T10:30:53.048123Z","iopub.status.idle":"2022-05-12T10:31:00.985117Z","shell.execute_reply.started":"2022-05-12T10:30:53.048087Z","shell.execute_reply":"2022-05-12T10:31:00.984287Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Cross-validator (Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.)\n(https://machinelearningmastery.com/k-fold-cross-validation/#:~:text=Cross%2Dvalidation%20is%20primarily%20used,the%20training%20of%20the%20model.)\nn-slips:Number of folds. Must be at least\n2. n_repeats - Number of times cross-validator needs to be repeated.\n3. random_state - Controls the generation of the random states for each repetition. Pass an int for reproducible output across multiple function calls\n evaluate the model on the dataset","metadata":{}},{"cell_type":"markdown","source":"The mean absolute error (MAE) of the model across all repeats and folds. The scikit-learn library makes the MAE negative so that it is maximized instead of minimized. This means that larger negative MAE are better and a perfect model has a MAE of 0. Mean Absolute Error is a model evaluation metric used with regression models. The mean absolute error of a model with respect to a test set is the mean of the absolute values of the individual prediction errors on over all instances in the test set. Each prediction error is the difference between the true value and the predicted value for the instance. (https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_525)","metadata":{}},{"cell_type":"code","source":" # make predictions using gradient boosting for classification\nmodel = GradientBoostingClassifier()\n#fit the model on the whole dataset\nmodel.fit(x,y)\n#make a single prediction\nrow=[53,46]\nyhat=model.predict([row])\nprint('Prediction: %d' % yhat[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/","metadata":{}},{"cell_type":"markdown","source":"## Gradient Boosting for Regression\n","metadata":{}},{"cell_type":"markdown","source":"The idea of gradient boosting is to improve weak learners and create a final combined prediction model. Decision trees are mainly used as base learners in this algorithm. The weak learner is identified by the gradient in the loss function. The prediction of a weak learner is compared to actual value and error is calculated. Based on this error, the model can determine the gradient and change the parameters to decrease the error rate in the next training. (https://www.datatechnotes.com/2019/06/gradient-boosting-regression-example-in.html)","metadata":{}},{"cell_type":"code","source":"#data=data.drop([\"PRCP\"],axis=1)\n\n# 2 Converting object into datetime to extract day, month and year\nfrom datetime import datetime\ndata[\"DATE\"]=pd.to_datetime(data[\"DATE\"], format= \"%Y-%m-%d\")\n\n# Extract day, month and year\ndata[\"DAY\"]=data[\"DATE\"].dt.day\ndata[\"MONTH\"]=data[\"DATE\"].dt.month\ndata[\"YEAR\"]=data[\"DATE\"].dt.year\ndata=data.drop([\"DATE\"], axis=1)\n\n#Rearrange columns\ndata=data[[\"DAY\", \"MONTH\", \"YEAR\", \"TMAX\", \"TMIN\", \"RAIN\"]]\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:01.689276Z","iopub.execute_input":"2022-05-12T10:31:01.689798Z","iopub.status.idle":"2022-05-12T10:31:01.738368Z","shell.execute_reply.started":"2022-05-12T10:31:01.689721Z","shell.execute_reply":"2022-05-12T10:31:01.737536Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Make a prediction of TMAX on a particular day\n","metadata":{}},{"cell_type":"code","source":"x=data.iloc[:,:-3].values\ny= data.iloc[:,-3].values","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:01.739606Z","iopub.execute_input":"2022-05-12T10:31:01.739823Z","iopub.status.idle":"2022-05-12T10:31:01.745253Z","shell.execute_reply.started":"2022-05-12T10:31:01.739792Z","shell.execute_reply":"2022-05-12T10:31:01.744007Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:01.746748Z","iopub.execute_input":"2022-05-12T10:31:01.747256Z","iopub.status.idle":"2022-05-12T10:31:01.761567Z","shell.execute_reply.started":"2022-05-12T10:31:01.747203Z","shell.execute_reply":"2022-05-12T10:31:01.760209Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train[:]=sc.fit_transform(x_train[:])\nx_test[:] = sc.transform(x_test[:])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:01.763131Z","iopub.execute_input":"2022-05-12T10:31:01.763702Z","iopub.status.idle":"2022-05-12T10:31:01.777293Z","shell.execute_reply.started":"2022-05-12T10:31:01.763643Z","shell.execute_reply":"2022-05-12T10:31:01.776582Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#x_train\n#y_test","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:01.778871Z","iopub.execute_input":"2022-05-12T10:31:01.780429Z","iopub.status.idle":"2022-05-12T10:31:01.788073Z","shell.execute_reply.started":"2022-05-12T10:31:01.779194Z","shell.execute_reply":"2022-05-12T10:31:01.787204Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RepeatedKFold\n\n\n# define the model\nmodel = GradientBoostingRegressor()\n# define the evaluation procedure\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model\nn_scores = cross_val_score(model, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# report performance\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:01.789886Z","iopub.execute_input":"2022-05-12T10:31:01.790424Z","iopub.status.idle":"2022-05-12T10:31:06.906491Z","shell.execute_reply.started":"2022-05-12T10:31:01.790367Z","shell.execute_reply":"2022-05-12T10:31:06.905656Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Difference between K-Fold and Stratified CV\nKFold is a cross-validator that divides the dataset into k folds. Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label.","metadata":{}},{"cell_type":"markdown","source":"## Predict\n","metadata":{}},{"cell_type":"code","source":"# define the model\nmodel = GradientBoostingRegressor()\n# fit the model on the whole dataset\nmodel.fit(x, y)\n# make a single prediction\nrow=[1,1,2022]\nyhat = model.predict([row])\n# summarize prediction\nprint('Prediction: %d' % yhat[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:06.907492Z","iopub.execute_input":"2022-05-12T10:31:06.907757Z","iopub.status.idle":"2022-05-12T10:31:07.319183Z","shell.execute_reply.started":"2022-05-12T10:31:06.907710Z","shell.execute_reply":"2022-05-12T10:31:07.318414Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#x_ax = range(len(y_test))\n#plt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\n#plt.plot(x_ax, yhat, lw=0.8, color=\"red\", label=\"predicted\")\n#plt.legend()\n#plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:07.320443Z","iopub.execute_input":"2022-05-12T10:31:07.320735Z","iopub.status.idle":"2022-05-12T10:31:07.324351Z","shell.execute_reply.started":"2022-05-12T10:31:07.320684Z","shell.execute_reply":"2022-05-12T10:31:07.323621Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, presort='auto', random_state=None,\n             subsample=1.0, verbose=0, warm_start=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:07.325875Z","iopub.execute_input":"2022-05-12T10:31:07.326171Z","iopub.status.idle":"2022-05-12T10:31:07.340112Z","shell.execute_reply.started":"2022-05-12T10:31:07.326080Z","shell.execute_reply":"2022-05-12T10:31:07.339311Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Number of decision trees in the ensemble is the important hyperparameter of the Gradient Boosting (n_estimators)\ndecision trees are added to the model sequentially in an effort to correct and improve upon the predictions made by prior trees. As such, more trees is often better. \n\n\nThe number of trees must also be balanced with the learning rate, e.g. more trees may require a smaller learning rate, fewer trees may require a larger learning rate.","metadata":{}},{"cell_type":"markdown","source":"## Learning Rate and Logarithmic loss","metadata":{}},{"cell_type":"code","source":"\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:07.341074Z","iopub.execute_input":"2022-05-12T10:31:07.341390Z","iopub.status.idle":"2022-05-12T10:31:07.349226Z","shell.execute_reply.started":"2022-05-12T10:31:07.341354Z","shell.execute_reply":"2022-05-12T10:31:07.348383Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib\nfrom matplotlib import pyplot\nmatplotlib.use('Agg')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:36:28.769946Z","iopub.execute_input":"2022-05-12T10:36:28.770300Z","iopub.status.idle":"2022-05-12T10:36:28.778337Z","shell.execute_reply.started":"2022-05-12T10:36:28.770247Z","shell.execute_reply":"2022-05-12T10:36:28.777387Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"data= pd.read_csv('../input/seattleWeather_1948-2017.csv')\ndata=data.drop([\"PRCP\"],axis=1)\ndata.dropna(inplace=True)\nx=data.iloc[:,1:-1]\ny=data.iloc[:,-1] \nlabel_encoded_y = LabelEncoder().fit_transform(y)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:38:04.290742Z","iopub.execute_input":"2022-05-12T10:38:04.291413Z","iopub.status.idle":"2022-05-12T10:38:04.337351Z","shell.execute_reply.started":"2022-05-12T10:38:04.291359Z","shell.execute_reply":"2022-05-12T10:38:04.336639Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"We can use the grid search capability in scikit-learn to evaluate the effect on logarithmic loss of training a gradient boosting model with different learning rate values.\n\n","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier()\nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nparam_grid = dict(learning_rate=learning_rate)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(x, label_encoded_y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n   \n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:43:42.618034Z","iopub.execute_input":"2022-05-12T10:43:42.618343Z","iopub.status.idle":"2022-05-12T10:44:00.751489Z","shell.execute_reply.started":"2022-05-12T10:43:42.618308Z","shell.execute_reply":"2022-05-12T10:44:00.750085Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"learning_rate = 0.1 is the best result --> the default number of trees is perfect for this model. We don't need to increase the number of trees","metadata":{}},{"cell_type":"markdown","source":"Smaller learning rates generally require more trees to be added to the model. An interesting link to the YouTube that explains the Gradient Boosting: https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:49:53.865468Z","iopub.execute_input":"2022-05-12T10:49:53.865747Z","iopub.status.idle":"2022-05-12T10:49:53.872344Z","shell.execute_reply.started":"2022-05-12T10:49:53.865711Z","shell.execute_reply":"2022-05-12T10:49:53.871358Z"}}},{"cell_type":"markdown","source":"## Learning Rate & Number of Trees","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier()\nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nn_estimators=[100,200,300,400,500]\nparam_grid = dict(learning_rate=learning_rate, n_estimators = n_estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(x, label_encoded_y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:54:15.730287Z","iopub.execute_input":"2022-05-12T10:54:15.730598Z","iopub.status.idle":"2022-05-12T10:58:29.794915Z","shell.execute_reply.started":"2022-05-12T10:54:15.730562Z","shell.execute_reply":"2022-05-12T10:58:29.793894Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"As we can see the most optimal is n_estimator = 100 with the learning_rate =0.1 (which is by default)","metadata":{}},{"cell_type":"markdown","source":"## Number of Trees ","metadata":{}},{"cell_type":"code","source":"model = GradientBoostingClassifier()\nn_estimators=[10,50,100,500,1000,5000]\nparam_grid = dict(n_estimators = n_estimators)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, x, label_encoded_y, scoring='accuracy', cv=cv, n_jobs=-1)\ngrid_result = grid_search.fit(x, label_encoded_y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T12:55:35.296292Z","iopub.execute_input":"2022-05-12T12:55:35.296870Z","iopub.status.idle":"2022-05-12T13:00:07.836093Z","shell.execute_reply.started":"2022-05-12T12:55:35.296821Z","shell.execute_reply":"2022-05-12T13:00:07.834692Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":" - param_griddict or list of dictionaries. Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=param_griddict%20or%20list%20of,any%20sequence%20of%20parameter%20settings.) \n - RepeatedStratifiedKFold --> \n     - n_splits - number of folds  - 10 folds (fold --> the folds are made by preserving the percentage of samples for each class)\n     - n_repeats - number of times cross-validator needs to be repeated - 3 times\n     - random_state - controls the generation of the random states for each repeatition.\n - cross_val_score - evaluates a score by cross-validation \n     - model (Gradient Boosting)\n     - x, y (label encoded (1,0)\n     - scoring - in our case we use the accuracy scoring\n     - cv (cross-validation indicator)\n     - n_jobs - number of jobs to run in parallel. -1 means using all jobs\n - grid_search - Exhaustive search over specified parameter values for an estimator.\n\n - The grid.best_score_ is the average of all cv folds for a single combination of the parameters you specify in the tuned_params.\n - grid_result.cv_results_[\"mean_test_score\"] - The mean_test_score that sklearn returns is the mean calculated on all samples where each sample has the same weight.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Sub-sample \nUsing fewer samples introduces more variance for each tree, although it can improve the overall performance of the model. \nThe number of samples used to fit each tree is specified by the “subsample” argument and can be set to a fraction of the training dataset size. By default, it is set to 1.0 to use the entire training dataset.\n\n(https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/)\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom numpy import arange\n# get a list of models to evaluate\ndef get_models():\n\tmodels = dict()\n\t# explore sample ratio from 10% to 100% in 10% increments\n\tfor i in arange(0.1, 1.1, 0.1):\n\t\tkey = '%.1f' % i\n\t\tmodels[key] = GradientBoostingClassifier(subsample=i)\n\treturn models\n \n# evaluate a given model using cross-validation\ndef evaluate_model(model, x, label_encoded_y):\n\t# define the evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\t# evaluate the model and collect the results\n\tscores = cross_val_score(model, x, label_encoded_y, scoring='accuracy', cv=cv, n_jobs=-1)\n\treturn scores\n \n# define dataset\n # get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\t# evaluate the model\n\tscores = evaluate_model(model, x, label_encoded_y)\n\t# store the results\n\tresults.append(scores)\n\tnames.append(name)\n\t# summarize the performance along the way\n\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:41:58.175835Z","iopub.execute_input":"2022-05-12T13:41:58.176432Z","iopub.status.idle":"2022-05-12T13:43:42.173280Z","shell.execute_reply.started":"2022-05-12T13:41:58.176384Z","shell.execute_reply":"2022-05-12T13:43:42.171744Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\n\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:45:19.986724Z","iopub.execute_input":"2022-05-12T13:45:19.987245Z","iopub.status.idle":"2022-05-12T13:45:20.243589Z","shell.execute_reply.started":"2022-05-12T13:45:19.987172Z","shell.execute_reply":"2022-05-12T13:45:20.242618Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Number of Features (max_features)\nThe number of features used to fit each decision tree can be varied.\n\nLike changing the number of samples, changing the number of features introduces additional variance into the model, which may improve performance, although it might require an increase in the number of trees.\n\n defaults to all features in the training dataset.","metadata":{}},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom numpy import arange\nfrom sklearn.datasets import make_classification\n\n# get the dataset\ndef get_dataset():\n\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n\treturn X, y\n\n# get a list of models to evaluate\ndef get_models():\n\tmodels = dict()\n\t# explore number of features from 1 to 20\n\tfor i in range(1,21):\n\t\tmodels[str(i)] = GradientBoostingClassifier(max_features=i)\n\treturn models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n\t# define the evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\t# evaluate the model and collect the results\n\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n\treturn scores\n\n# define dataset\nX, y = get_dataset()\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\t# evaluate the model\n\tscores = evaluate_model(model, X, y)\n\t# store the results\n\tresults.append(scores)\n\tnames.append(name)\n\t# summarize the performance along the way\n\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\n","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:51:50.929437Z","iopub.execute_input":"2022-05-12T13:51:50.929977Z","iopub.status.idle":"2022-05-12T13:52:33.891049Z","shell.execute_reply.started":"2022-05-12T13:51:50.929911Z","shell.execute_reply":"2022-05-12T13:52:33.890167Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"## Grid Search Hyperparameters (n_estimators, learning_rate, subsample, max_depth)","metadata":{}},{"cell_type":"code","source":"model = GradientBoostingClassifier()\ngrid=dict()\ngrid['n_estimators'] = [10, 50, 100, 500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\ngrid['subsample'] = [0.5, 0.7, 1.0]\ngrid['max_depth'] = [3, 7, 9]\n\n#evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n#define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid,  cv=cv, scoring='accuracy',n_jobs=-1)\n#execute the grid search procedure\ngrid_result = grid_search.fit(x, label_encoded_y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T14:00:22.889697Z","iopub.execute_input":"2022-05-12T14:00:22.890014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}